{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81cacd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trader Behavior Insights — Initial Load & Sanity Check\n",
    "\n",
    "#Load the Hyperliquid trade data and the Bitcoin Fear & Greed index, parse datetimes, and do an initial data audit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0e7fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pd.set_option('display.max_columns', 120)\n",
    "pd.set_option('display.width', 150)\n",
    "sns.set(style='whitegrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "418b8d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting files at:\n",
      "c:\\Users\\Aditya\\Desktop\\datascience\\notebooks\\data\\historical.csv\n",
      "c:\\Users\\Aditya\\Desktop\\datascience\\notebooks\\data\\fear_greed.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\Aditya\\\\Desktop\\\\datascience\\\\notebooks\\\\data\\\\historical.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(trades_path)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(fg_path)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m trades = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrades_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m fg = pd.read_csv(fg_path)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mLoaded:\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aditya\\Desktop\\datascience\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aditya\\Desktop\\datascience\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aditya\\Desktop\\datascience\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aditya\\Desktop\\datascience\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aditya\\Desktop\\datascience\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\Aditya\\\\Desktop\\\\datascience\\\\notebooks\\\\data\\\\historical.csv'"
     ]
    }
   ],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), 'data')   # should point to datascience/data\n",
    "trades_path = os.path.join(DATA_DIR, 'historical.csv')\n",
    "fg_path = os.path.join(DATA_DIR, 'fear_greed.csv')\n",
    "\n",
    "print('Expecting files at:')\n",
    "print(trades_path)\n",
    "print(fg_path)\n",
    "\n",
    "trades = pd.read_csv(trades_path)\n",
    "fg = pd.read_csv(fg_path)\n",
    "\n",
    "print('Loaded:')\n",
    "print('trades:', trades.shape)\n",
    "print('fg:', fg.shape)\n",
    "\n",
    "# show first few rows\n",
    "display(trades.head())\n",
    "display(fg.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ee13aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fear_greed_index.csv', 'historical_data.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(os.path.join(os.path.dirname(os.getcwd()), \"data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3edc03f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for files at:\n",
      "c:\\Users\\Aditya\\Desktop\\datascience\\data\\historical_data.csv\n",
      "c:\\Users\\Aditya\\Desktop\\datascience\\data\\fear_greed_index.csv\n",
      "Loaded successfully:\n",
      "Trades dataset shape: (211224, 16)\n",
      "Fear & Greed dataset shape: (2644, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account</th>\n",
       "      <th>Coin</th>\n",
       "      <th>Execution Price</th>\n",
       "      <th>Size Tokens</th>\n",
       "      <th>Size USD</th>\n",
       "      <th>Side</th>\n",
       "      <th>Timestamp IST</th>\n",
       "      <th>Start Position</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Closed PnL</th>\n",
       "      <th>Transaction Hash</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Crossed</th>\n",
       "      <th>Fee</th>\n",
       "      <th>Trade ID</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>7.9769</td>\n",
       "      <td>986.87</td>\n",
       "      <td>7872.16</td>\n",
       "      <td>BUY</td>\n",
       "      <td>02-12-2024 22:50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0xec09451986a1874e3a980418412fcd0201f500c95bac...</td>\n",
       "      <td>52017706630</td>\n",
       "      <td>True</td>\n",
       "      <td>0.345404</td>\n",
       "      <td>8.950000e+14</td>\n",
       "      <td>1.730000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>7.9800</td>\n",
       "      <td>16.00</td>\n",
       "      <td>127.68</td>\n",
       "      <td>BUY</td>\n",
       "      <td>02-12-2024 22:50</td>\n",
       "      <td>986.524596</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0xec09451986a1874e3a980418412fcd0201f500c95bac...</td>\n",
       "      <td>52017706630</td>\n",
       "      <td>True</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>4.430000e+14</td>\n",
       "      <td>1.730000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>7.9855</td>\n",
       "      <td>144.09</td>\n",
       "      <td>1150.63</td>\n",
       "      <td>BUY</td>\n",
       "      <td>02-12-2024 22:50</td>\n",
       "      <td>1002.518996</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0xec09451986a1874e3a980418412fcd0201f500c95bac...</td>\n",
       "      <td>52017706630</td>\n",
       "      <td>True</td>\n",
       "      <td>0.050431</td>\n",
       "      <td>6.600000e+14</td>\n",
       "      <td>1.730000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>7.9874</td>\n",
       "      <td>142.98</td>\n",
       "      <td>1142.04</td>\n",
       "      <td>BUY</td>\n",
       "      <td>02-12-2024 22:50</td>\n",
       "      <td>1146.558564</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0xec09451986a1874e3a980418412fcd0201f500c95bac...</td>\n",
       "      <td>52017706630</td>\n",
       "      <td>True</td>\n",
       "      <td>0.050043</td>\n",
       "      <td>1.080000e+15</td>\n",
       "      <td>1.730000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>7.9894</td>\n",
       "      <td>8.73</td>\n",
       "      <td>69.75</td>\n",
       "      <td>BUY</td>\n",
       "      <td>02-12-2024 22:50</td>\n",
       "      <td>1289.488521</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0xec09451986a1874e3a980418412fcd0201f500c95bac...</td>\n",
       "      <td>52017706630</td>\n",
       "      <td>True</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>1.050000e+15</td>\n",
       "      <td>1.730000e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Account  Coin  Execution Price  Size Tokens  Size USD Side     Timestamp IST  Start Position Direction  \\\n",
       "0  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9769       986.87   7872.16  BUY  02-12-2024 22:50        0.000000       Buy   \n",
       "1  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9800        16.00    127.68  BUY  02-12-2024 22:50      986.524596       Buy   \n",
       "2  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9855       144.09   1150.63  BUY  02-12-2024 22:50     1002.518996       Buy   \n",
       "3  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9874       142.98   1142.04  BUY  02-12-2024 22:50     1146.558564       Buy   \n",
       "4  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9894         8.73     69.75  BUY  02-12-2024 22:50     1289.488521       Buy   \n",
       "\n",
       "   Closed PnL                                   Transaction Hash     Order ID  Crossed       Fee      Trade ID     Timestamp  \n",
       "0         0.0  0xec09451986a1874e3a980418412fcd0201f500c95bac...  52017706630     True  0.345404  8.950000e+14  1.730000e+12  \n",
       "1         0.0  0xec09451986a1874e3a980418412fcd0201f500c95bac...  52017706630     True  0.005600  4.430000e+14  1.730000e+12  \n",
       "2         0.0  0xec09451986a1874e3a980418412fcd0201f500c95bac...  52017706630     True  0.050431  6.600000e+14  1.730000e+12  \n",
       "3         0.0  0xec09451986a1874e3a980418412fcd0201f500c95bac...  52017706630     True  0.050043  1.080000e+15  1.730000e+12  \n",
       "4         0.0  0xec09451986a1874e3a980418412fcd0201f500c95bac...  52017706630     True  0.003055  1.050000e+15  1.730000e+12  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "      <th>classification</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1517463000</td>\n",
       "      <td>30</td>\n",
       "      <td>Fear</td>\n",
       "      <td>2018-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1517549400</td>\n",
       "      <td>15</td>\n",
       "      <td>Extreme Fear</td>\n",
       "      <td>2018-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1517635800</td>\n",
       "      <td>40</td>\n",
       "      <td>Fear</td>\n",
       "      <td>2018-02-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1517722200</td>\n",
       "      <td>24</td>\n",
       "      <td>Extreme Fear</td>\n",
       "      <td>2018-02-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1517808600</td>\n",
       "      <td>11</td>\n",
       "      <td>Extreme Fear</td>\n",
       "      <td>2018-02-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  value classification        date\n",
       "0  1517463000     30           Fear  2018-02-01\n",
       "1  1517549400     15   Extreme Fear  2018-02-02\n",
       "2  1517635800     40           Fear  2018-02-03\n",
       "3  1517722200     24   Extreme Fear  2018-02-04\n",
       "4  1517808600     11   Extreme Fear  2018-02-05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Go one level up from /notebooks to reach /datascience\n",
    "ROOT_DIR = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "\n",
    "trades_path = os.path.join(DATA_DIR, 'historical_data.csv')\n",
    "fg_path = os.path.join(DATA_DIR, 'fear_greed_index.csv')\n",
    "\n",
    "print(\"Looking for files at:\")\n",
    "print(trades_path)\n",
    "print(fg_path)\n",
    "\n",
    "# Load the data\n",
    "trades = pd.read_csv(trades_path)\n",
    "fg = pd.read_csv(fg_path)\n",
    "\n",
    "print(\"Loaded successfully:\")\n",
    "print(\"Trades dataset shape:\", trades.shape)\n",
    "print(\"Fear & Greed dataset shape:\", fg.shape)\n",
    "\n",
    "display(trades.head())\n",
    "display(fg.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa59652e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trades columns: ['Account', 'Coin', 'Execution Price', 'Size Tokens', 'Size USD', 'Side', 'Timestamp IST', 'Start Position', 'Direction', 'Closed PnL', 'Transaction Hash', 'Order ID', 'Crossed', 'Fee', 'Trade ID', 'Timestamp']\n",
      "FearGreed columns: ['timestamp', 'value', 'classification', 'date']\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No obvious time column found in trades. Check trades.columns above.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trade_time_col \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo obvious time column found in trades. Check trades.columns above.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing trade time column:\u001b[39m\u001b[33m\"\u001b[39m, trade_time_col)\n\u001b[32m     22\u001b[39m trades[trade_time_col] = pd.to_datetime(trades[trade_time_col], utc=\u001b[38;5;28;01mTrue\u001b[39;00m, errors=\u001b[33m'\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: No obvious time column found in trades. Check trades.columns above."
     ]
    }
   ],
   "source": [
    "# Step 5 — Parse datetimes, inspect columns, and basic cleaning\n",
    "import numpy as np\n",
    "\n",
    "# 1) Show column names (so you know what to use later)\n",
    "print(\"Trades columns:\", list(trades.columns))\n",
    "print(\"FearGreed columns:\", list(fg.columns))\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 2) Parse trade timestamp\n",
    "# Try common timestamp column names; adapt if your column names differ.\n",
    "time_col_candidates = ['time', 'timestamp', 'Time', 'date', 'Date']\n",
    "trade_time_col = None\n",
    "for c in time_col_candidates:\n",
    "    if c in trades.columns:\n",
    "        trade_time_col = c\n",
    "        break\n",
    "\n",
    "if trade_time_col is None:\n",
    "    raise ValueError(\"No obvious time column found in trades. Check trades.columns above.\")\n",
    "\n",
    "print(\"Using trade time column:\", trade_time_col)\n",
    "trades[trade_time_col] = pd.to_datetime(trades[trade_time_col], utc=True, errors='coerce')\n",
    "\n",
    "# 3) Parse Fear&Greed date column\n",
    "fg_date_col = None\n",
    "for c in ['Date', 'date', 'day', 'datetime']:\n",
    "    if c in fg.columns:\n",
    "        fg_date_col = c\n",
    "        break\n",
    "\n",
    "if fg_date_col is None:\n",
    "    # If no explicit date column, try first column\n",
    "    fg_date_col = fg.columns[0]\n",
    "    print(\"No standard Date column found in FG; using first column:\", fg_date_col)\n",
    "else:\n",
    "    print(\"Using FG date column:\", fg_date_col)\n",
    "\n",
    "fg[fg_date_col] = pd.to_datetime(fg[fg_date_col], errors='coerce')\n",
    "\n",
    "# 4) Create normalized date-only fields for merging\n",
    "trades['trade_date'] = trades[trade_time_col].dt.date\n",
    "fg['fg_date'] = fg[fg_date_col].dt.date\n",
    "\n",
    "# 5) Show missingness and basic stats for key numerical columns if present\n",
    "print(\"\\nTrades head (preview):\")\n",
    "display(trades.head())\n",
    "print(\"\\nFear&Greed head (preview):\")\n",
    "display(fg.head())\n",
    "\n",
    "print(\"\\nMissing values (top 10) in trades:\")\n",
    "print(trades.isnull().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "print(\"\\nMissing values (top 10) in fear&greed:\")\n",
    "print(fg.isnull().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "# 6) Ensure numeric columns are numeric (common names)\n",
    "num_cols = ['execution price','execution_price','price','size','closedPnL','closed_pnl','leverage','Leverage']\n",
    "for col in num_cols:\n",
    "    if col in trades.columns:\n",
    "        trades[col] = pd.to_numeric(trades[col], errors='coerce')\n",
    "\n",
    "# 7) Print basic ranges for numeric columns we found\n",
    "found_num = [c for c in num_cols if c in trades.columns]\n",
    "if found_num:\n",
    "    print(\"\\nNumeric column summaries:\")\n",
    "    display(trades[found_num].describe().T)\n",
    "else:\n",
    "    print(\"\\nNo standard numeric columns found among expected names; you'll have to map them manually later.\")\n",
    "\n",
    "# 8) Quick check: how many trades have a matching FG date (non-null after merge preview)\n",
    "fg_dates_set = set(fg['fg_date'].dropna().unique())\n",
    "matched_mask = trades['trade_date'].isin(fg_dates_set)\n",
    "print(\"\\nTrades with sentiment available (approx):\", matched_mask.sum(), \"/\", len(trades))\n",
    "print(\"Fraction with sentiment:\", matched_mask.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d0c5f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Account', 'Coin', 'Execution Price', 'Size Tokens', 'Size USD', 'Side', 'Timestamp IST', 'Start Position', 'Direction', 'Closed PnL',\n",
      "       'Transaction Hash', 'Order ID', 'Crossed', 'Fee', 'Trade ID', 'Timestamp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "trades = pd.read_csv(r\"c:\\Users\\Aditya\\Desktop\\datascience\\data\\historical_data.csv\")\n",
    "print(trades.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f9ff741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using timestamp column: Timestamp\n",
      "\n",
      "Trades preview (selected cols):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_11236\\2636550268.py:21: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  trades[time_col] = pd.to_datetime(trades[time_col], errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_11236\\2636550268.py:73: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  fg[fg_date_col] = pd.to_datetime(fg[fg_date_col], errors='coerce', infer_datetime_format=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account</th>\n",
       "      <th>Coin</th>\n",
       "      <th>execution_price</th>\n",
       "      <th>size_tokens</th>\n",
       "      <th>size_usd</th>\n",
       "      <th>notional</th>\n",
       "      <th>closed_pnl</th>\n",
       "      <th>return_pct</th>\n",
       "      <th>profitable</th>\n",
       "      <th>trade_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>7.9769</td>\n",
       "      <td>986.87</td>\n",
       "      <td>7872.16</td>\n",
       "      <td>7872.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>7.9800</td>\n",
       "      <td>16.00</td>\n",
       "      <td>127.68</td>\n",
       "      <td>127.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>7.9855</td>\n",
       "      <td>144.09</td>\n",
       "      <td>1150.63</td>\n",
       "      <td>1150.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>7.9874</td>\n",
       "      <td>142.98</td>\n",
       "      <td>1142.04</td>\n",
       "      <td>1142.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>7.9894</td>\n",
       "      <td>8.73</td>\n",
       "      <td>69.75</td>\n",
       "      <td>69.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>7.9900</td>\n",
       "      <td>1.41</td>\n",
       "      <td>11.27</td>\n",
       "      <td>11.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>7.9934</td>\n",
       "      <td>144.09</td>\n",
       "      <td>1151.77</td>\n",
       "      <td>1151.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>34.00</td>\n",
       "      <td>272.00</td>\n",
       "      <td>272.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Account  Coin  execution_price  size_tokens  size_usd  notional  closed_pnl  return_pct  profitable  \\\n",
       "0  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9769       986.87   7872.16   7872.16         0.0         0.0           0   \n",
       "1  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9800        16.00    127.68    127.68         0.0         0.0           0   \n",
       "2  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9855       144.09   1150.63   1150.63         0.0         0.0           0   \n",
       "3  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9874       142.98   1142.04   1142.04         0.0         0.0           0   \n",
       "4  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9894         8.73     69.75     69.75         0.0         0.0           0   \n",
       "5  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9900         1.41     11.27     11.27         0.0         0.0           0   \n",
       "6  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9934       144.09   1151.77   1151.77         0.0         0.0           0   \n",
       "7  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           8.0000        34.00    272.00    272.00         0.0         0.0           0   \n",
       "\n",
       "   trade_date  \n",
       "0  1970-01-01  \n",
       "1  1970-01-01  \n",
       "2  1970-01-01  \n",
       "3  1970-01-01  \n",
       "4  1970-01-01  \n",
       "5  1970-01-01  \n",
       "6  1970-01-01  \n",
       "7  1970-01-01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numeric summaries:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>execution_price</th>\n",
       "      <td>211224.0</td>\n",
       "      <td>11414.723350</td>\n",
       "      <td>29447.654868</td>\n",
       "      <td>4.530000e-06</td>\n",
       "      <td>4.8547</td>\n",
       "      <td>18.280</td>\n",
       "      <td>101.580000</td>\n",
       "      <td>1.090040e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size_tokens</th>\n",
       "      <td>211224.0</td>\n",
       "      <td>4623.364979</td>\n",
       "      <td>104272.889530</td>\n",
       "      <td>8.740000e-07</td>\n",
       "      <td>2.9400</td>\n",
       "      <td>32.000</td>\n",
       "      <td>187.902500</td>\n",
       "      <td>1.582244e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size_usd</th>\n",
       "      <td>211224.0</td>\n",
       "      <td>5639.451210</td>\n",
       "      <td>36575.138546</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>193.7900</td>\n",
       "      <td>597.045</td>\n",
       "      <td>2058.960000</td>\n",
       "      <td>3.921431e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>notional</th>\n",
       "      <td>211224.0</td>\n",
       "      <td>5639.451210</td>\n",
       "      <td>36575.138546</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>193.7900</td>\n",
       "      <td>597.045</td>\n",
       "      <td>2058.960000</td>\n",
       "      <td>3.921431e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>closed_pnl</th>\n",
       "      <td>211224.0</td>\n",
       "      <td>48.749001</td>\n",
       "      <td>919.164828</td>\n",
       "      <td>-1.179901e+05</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.792797</td>\n",
       "      <td>1.353291e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_pct</th>\n",
       "      <td>211181.0</td>\n",
       "      <td>0.018995</td>\n",
       "      <td>0.845813</td>\n",
       "      <td>-3.844064e+02</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010323</td>\n",
       "      <td>3.403550e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count          mean            std           min       25%      50%          75%           max\n",
       "execution_price  211224.0  11414.723350   29447.654868  4.530000e-06    4.8547   18.280   101.580000  1.090040e+05\n",
       "size_tokens      211224.0   4623.364979  104272.889530  8.740000e-07    2.9400   32.000   187.902500  1.582244e+07\n",
       "size_usd         211224.0   5639.451210   36575.138546  0.000000e+00  193.7900  597.045  2058.960000  3.921431e+06\n",
       "notional         211224.0   5639.451210   36575.138546  0.000000e+00  193.7900  597.045  2058.960000  3.921431e+06\n",
       "closed_pnl       211224.0     48.749001     919.164828 -1.179901e+05    0.0000    0.000     5.792797  1.353291e+05\n",
       "return_pct       211181.0      0.018995       0.845813 -3.844064e+02    0.0000    0.000     0.010323  3.403550e+00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fear&Greed preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "      <th>classification</th>\n",
       "      <th>date</th>\n",
       "      <th>fg_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1517463000</td>\n",
       "      <td>30</td>\n",
       "      <td>Fear</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2018-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1517549400</td>\n",
       "      <td>15</td>\n",
       "      <td>Extreme Fear</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>2018-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1517635800</td>\n",
       "      <td>40</td>\n",
       "      <td>Fear</td>\n",
       "      <td>2018-02-03</td>\n",
       "      <td>2018-02-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1517722200</td>\n",
       "      <td>24</td>\n",
       "      <td>Extreme Fear</td>\n",
       "      <td>2018-02-04</td>\n",
       "      <td>2018-02-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1517808600</td>\n",
       "      <td>11</td>\n",
       "      <td>Extreme Fear</td>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>2018-02-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  value classification       date     fg_date\n",
       "0  1517463000     30           Fear 2018-02-01  2018-02-01\n",
       "1  1517549400     15   Extreme Fear 2018-02-02  2018-02-02\n",
       "2  1517635800     40           Fear 2018-02-03  2018-02-03\n",
       "3  1517722200     24   Extreme Fear 2018-02-04  2018-02-04\n",
       "4  1517808600     11   Extreme Fear 2018-02-05  2018-02-05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trades with sentiment available: 0 / 211224\n",
      "Fraction with sentiment: 0.0\n",
      "\n",
      "Timestamp parse stats:\n",
      "Total rows: 211224\n",
      "Parsed timestamps (non-null): 211224\n",
      "Null timestamps: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 5 (updated) — parse timestamps and basic cleaning specific to your columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# reload trades (safe to reload)\n",
    "trades = pd.read_csv(r\"c:\\Users\\Aditya\\Desktop\\datascience\\data\\historical_data.csv\")\n",
    "fg = pd.read_csv(r\"c:\\Users\\Aditya\\Desktop\\datascience\\data\\fear_greed_index.csv\")\n",
    "\n",
    "# Select timestamp column: prefer 'Timestamp' then 'Timestamp IST'\n",
    "if 'Timestamp' in trades.columns and trades['Timestamp'].notna().any():\n",
    "    time_col = 'Timestamp'\n",
    "elif 'Timestamp IST' in trades.columns and trades['Timestamp IST'].notna().any():\n",
    "    time_col = 'Timestamp IST'\n",
    "else:\n",
    "    raise ValueError(\"No usable Timestamp column found in trades.\")\n",
    "\n",
    "print(\"Using timestamp column:\", time_col)\n",
    "\n",
    "# Parse datetimes (try to infer format)\n",
    "trades[time_col] = pd.to_datetime(trades[time_col], errors='coerce', infer_datetime_format=True)\n",
    "\n",
    "# If Timestamp IST exists and Timestamp parsed as NaT for many rows, try parsing IST explicitly:\n",
    "if time_col == 'Timestamp' and trades[time_col].isna().mean() > 0.2 and 'Timestamp IST' in trades.columns:\n",
    "    print(\"Many nulls in 'Timestamp' — trying 'Timestamp IST' fallback parsing...\")\n",
    "    trades['Timestamp IST'] = pd.to_datetime(trades['Timestamp IST'], errors='coerce', infer_datetime_format=True)\n",
    "    # if fallback parsed better, use it\n",
    "    if trades['Timestamp IST'].notna().sum() > trades[time_col].notna().sum():\n",
    "        time_col = 'Timestamp IST'\n",
    "        trades[time_col] = trades['Timestamp IST']\n",
    "        print(\"Switched to:\", time_col)\n",
    "\n",
    "# create trade_date for daily merge\n",
    "trades['trade_date'] = trades[time_col].dt.date\n",
    "\n",
    "# Convert numeric columns (handle spaces/case)\n",
    "num_map = {\n",
    "    'Execution Price': 'execution_price',\n",
    "    'Size Tokens': 'size_tokens',\n",
    "    'Size USD': 'size_usd',\n",
    "    'Closed PnL': 'closed_pnl',\n",
    "    'Fee': 'fee'\n",
    "}\n",
    "for orig, col in num_map.items():\n",
    "    if orig in trades.columns:\n",
    "        trades[col] = pd.to_numeric(trades[orig], errors='coerce')\n",
    "    else:\n",
    "        trades[col] = np.nan\n",
    "\n",
    "# Prefer notional from Size USD; fall back to execution_price * size_tokens\n",
    "if 'size_usd' in trades.columns and trades['size_usd'].notna().sum() > 0:\n",
    "    trades['notional'] = trades['size_usd']\n",
    "else:\n",
    "    trades['notional'] = trades['execution_price'].abs() * trades['size_tokens'].abs()\n",
    "\n",
    "# closed pnl numeric\n",
    "trades['closed_pnl'] = trades['closed_pnl']  # already created above\n",
    "\n",
    "# return pct (safely)\n",
    "trades['return_pct'] = trades['closed_pnl'] / trades['notional'].replace({0: np.nan})\n",
    "trades['profitable'] = (trades['closed_pnl'] > 0).astype(int)\n",
    "\n",
    "# Process FG dates\n",
    "# detect date column in FG (common names)\n",
    "fg_date_col = None\n",
    "for c in ['Date','date','day','datetime']:\n",
    "    if c in fg.columns:\n",
    "        fg_date_col = c\n",
    "        break\n",
    "if fg_date_col is None:\n",
    "    fg_date_col = fg.columns[0]\n",
    "\n",
    "fg[fg_date_col] = pd.to_datetime(fg[fg_date_col], errors='coerce', infer_datetime_format=True)\n",
    "fg['fg_date'] = fg[fg_date_col].dt.date\n",
    "\n",
    "# Show outputs and sanity checks\n",
    "print(\"\\nTrades preview (selected cols):\")\n",
    "display(trades[['Account','Coin','execution_price','size_tokens','size_usd','notional','closed_pnl','return_pct','profitable','trade_date']].head(8))\n",
    "\n",
    "print(\"\\nNumeric summaries:\")\n",
    "display(trades[['execution_price','size_tokens','size_usd','notional','closed_pnl','return_pct']].describe().T)\n",
    "\n",
    "print(\"\\nFear&Greed preview:\")\n",
    "display(fg.head())\n",
    "\n",
    "# How many trades have a sentiment match available?\n",
    "fg_dates_set = set(fg['fg_date'].dropna().unique())\n",
    "matched = trades['trade_date'].isin(fg_dates_set)\n",
    "print(\"\\nTrades with sentiment available:\", int(matched.sum()), \"/\", len(trades))\n",
    "print(\"Fraction with sentiment:\", matched.mean())\n",
    "\n",
    "# Print counts of null timestamps parsed\n",
    "print(\"\\nTimestamp parse stats:\")\n",
    "print(\"Total rows:\", len(trades))\n",
    "print(\"Parsed timestamps (non-null):\", trades[time_col].notna().sum())\n",
    "print(\"Null timestamps:\", trades[time_col].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d17fa12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset shape: (211224, 30)\n",
      "\n",
      "Merged preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trade_date</th>\n",
       "      <th>Account</th>\n",
       "      <th>Coin</th>\n",
       "      <th>return_pct</th>\n",
       "      <th>profitable</th>\n",
       "      <th>fg_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trade_date                                     Account  Coin  return_pct  profitable fg_date\n",
       "0  1970-01-01  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107         0.0           0     NaN\n",
       "1  1970-01-01  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107         0.0           0     NaN\n",
       "2  1970-01-01  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107         0.0           0     NaN\n",
       "3  1970-01-01  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107         0.0           0     NaN\n",
       "4  1970-01-01  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107         0.0           0     NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null sentiment rows: 211224\n"
     ]
    }
   ],
   "source": [
    "# Step 6 — Merge trades with fear & greed sentiment\n",
    "\n",
    "# Rename FG columns for clarity if needed\n",
    "fg = fg.rename(columns={fg_date_col: \"fg_datetime\"})\n",
    "\n",
    "# We already created fg['fg_date'] and trades['trade_date'], so merge on date\n",
    "merged = trades.merge(\n",
    "    fg[['fg_date', 'fear_and_greed_index'] if 'fear_and_greed_index' in fg.columns else fg.columns],\n",
    "    left_on='trade_date',\n",
    "    right_on='fg_date',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Merged dataset shape:\", merged.shape)\n",
    "\n",
    "print(\"\\nMerged preview:\")\n",
    "display(merged[['trade_date','Account','Coin','return_pct','profitable','fear_and_greed_index' if 'fear_and_greed_index' in merged.columns else merged.columns[-1]]].head())\n",
    "\n",
    "print(\"\\nNull sentiment rows:\", merged['fear_and_greed_index' if 'fear_and_greed_index' in merged.columns else merged.columns[-1]].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa662214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged columns:\n",
      " ['Account', 'Coin', 'Execution Price', 'Size Tokens', 'Size USD', 'Side', 'Timestamp IST', 'Start Position', 'Direction', 'Closed PnL', 'Transaction Hash', 'Order ID', 'Crossed', 'Fee', 'Trade ID', 'Timestamp', 'trade_date', 'execution_price', 'size_tokens', 'size_usd', 'closed_pnl', 'fee', 'notional', 'return_pct', 'profitable', 'timestamp', 'value', 'classification', 'fg_datetime', 'fg_date']\n",
      "\n",
      "Looking for likely sentiment columns...\n",
      "Using sentiment column: classification\n",
      "First unique values in sentiment column (up to 20):\n",
      " ['nan']\n",
      "\n",
      "ML dataframe shape (rows with required features): (0, 36)\n",
      "\n",
      "Feature sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_sentiment_num</th>\n",
       "      <th>sentiment_lag_1</th>\n",
       "      <th>sentiment_lag_2</th>\n",
       "      <th>sentiment_roll_7</th>\n",
       "      <th>side_num</th>\n",
       "      <th>size_usd_norm</th>\n",
       "      <th>profitable</th>\n",
       "      <th>Account</th>\n",
       "      <th>trade_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [_sentiment_num, sentiment_lag_1, sentiment_lag_2, sentiment_roll_7, side_num, size_usd_norm, profitable, Account, trade_date]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Detect sentiment column and create features (safe, single cell)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# show merged columns to inspect\n",
    "print(\"Merged columns:\\n\", list(merged.columns))\n",
    "print(\"\\nLooking for likely sentiment columns...\")\n",
    "\n",
    "# candidates to search for (common names)\n",
    "candidates = [\n",
    "    'fear_and_greed_index','fear_greed_index','FearGreedIndex','Score','score',\n",
    "    'classification','Classification','Value','value','Index','index',\n",
    "    'fear_greed','Fear_Greed','fear_greed_score','fear_greed_value'\n",
    "]\n",
    "\n",
    "sentiment_col = None\n",
    "for c in candidates:\n",
    "    if c in merged.columns:\n",
    "        sentiment_col = c\n",
    "        break\n",
    "\n",
    "# if not found, try to find any column that contains 'fear' or 'greed' or 'score' (case-insensitive)\n",
    "if sentiment_col is None:\n",
    "    for col in merged.columns:\n",
    "        low = col.lower()\n",
    "        if ('fear' in low) or ('greed' in low) or ('score' in low):\n",
    "            sentiment_col = col\n",
    "            break\n",
    "\n",
    "if sentiment_col is None:\n",
    "    raise KeyError(\"Couldn't find a sentiment column in merged. Check merged.columns printed above.\")\n",
    "\n",
    "print(\"Using sentiment column:\", sentiment_col)\n",
    "\n",
    "# Ensure Timestamp column exists (we used it earlier)\n",
    "time_col = 'Timestamp' if 'Timestamp' in merged.columns else None\n",
    "if time_col is None:\n",
    "    # fallback: try 'Timestamp IST' or other\n",
    "    if 'Timestamp IST' in merged.columns:\n",
    "        time_col = 'Timestamp IST'\n",
    "    else:\n",
    "        raise KeyError(\"No Timestamp column found in merged. Check merged.columns.\")\n",
    "\n",
    "# Work on a copy\n",
    "df = merged.copy()\n",
    "\n",
    "# ensure timestamp parsed as datetime\n",
    "df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n",
    "\n",
    "# sort by Account + time so lag makes sense\n",
    "df = df.sort_values(['Account', time_col])\n",
    "\n",
    "# create trade_date if not exists\n",
    "if 'trade_date' not in df.columns:\n",
    "    df['trade_date'] = df[time_col].dt.date\n",
    "\n",
    "# create sentiment numeric column (if classification text exists, keep as-is)\n",
    "# If the sentiment column is text/classification (e.g., 'Fear'/'Greed'), create a numeric mapping too.\n",
    "if df[sentiment_col].dtype == object:\n",
    "    # show unique values (first 20)\n",
    "    uniq = pd.Series(df[sentiment_col].unique()).astype(str)\n",
    "    print(\"First unique values in sentiment column (up to 20):\\n\", uniq.head(20).tolist())\n",
    "    # try a simple mapping: Greed=1, Neutral=0, Fear=-1 (only if those values present)\n",
    "    mapping = {}\n",
    "    for val in uniq:\n",
    "        v = str(val).lower()\n",
    "        if 'greed' in v:\n",
    "            mapping[val] = 1\n",
    "        elif 'fear' in v:\n",
    "            mapping[val] = -1\n",
    "        elif 'neutral' in v or 'none' in v:\n",
    "            mapping[val] = 0\n",
    "    if mapping:\n",
    "        df['_sentiment_num'] = df[sentiment_col].map(mapping)\n",
    "        print(\"Applied text->numeric mapping for sentiment. Example mapping:\", mapping)\n",
    "    else:\n",
    "        # fallback: leave numeric column as NaN\n",
    "        df['_sentiment_num'] = np.nan\n",
    "else:\n",
    "    # numeric already\n",
    "    df['_sentiment_num'] = pd.to_numeric(df[sentiment_col], errors='coerce')\n",
    "\n",
    "# Lagged sentiment per account (use numeric version)\n",
    "df['sentiment_lag_1'] = df.groupby('Account')['_sentiment_num'].shift(1)\n",
    "df['sentiment_lag_2'] = df.groupby('Account')['_sentiment_num'].shift(2)\n",
    "\n",
    "# Rolling 7 (per account) on numeric sentiment\n",
    "df['sentiment_roll_7'] = df.groupby('Account')['_sentiment_num'].rolling(7, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "# side encoding (safe mapping)\n",
    "if 'Side' in df.columns:\n",
    "    df['side_num'] = df['Side'].map({'Buy': 1, 'Sell': -1}).fillna(0)\n",
    "else:\n",
    "    df['side_num'] = 0\n",
    "\n",
    "# ensure notional exists (we computed earlier as 'notional' possibly). If different name, try common ones\n",
    "if 'notional' not in df.columns:\n",
    "    if 'Size USD' in df.columns:\n",
    "        df['notional'] = pd.to_numeric(df['Size USD'], errors='coerce')\n",
    "    elif 'size_usd' in df.columns:\n",
    "        df['notional'] = pd.to_numeric(df['size_usd'], errors='coerce')\n",
    "    else:\n",
    "        # try compute from Execution Price * Size Tokens\n",
    "        if 'Execution Price' in df.columns and 'Size Tokens' in df.columns:\n",
    "            df['notional'] = pd.to_numeric(df['Execution Price'], errors='coerce').abs() * pd.to_numeric(df['Size Tokens'], errors='coerce').abs()\n",
    "        else:\n",
    "            df['notional'] = np.nan\n",
    "\n",
    "# closed pnl numeric (try common names)\n",
    "if 'Closed PnL' in df.columns:\n",
    "    df['closed_pnl'] = pd.to_numeric(df['Closed PnL'], errors='coerce')\n",
    "elif 'closed_pnl' not in df.columns:\n",
    "    df['closed_pnl'] = pd.to_numeric(df.get('Closed PnL', df.get('closed_pnl', np.nan)), errors='coerce')\n",
    "\n",
    "# return pct\n",
    "df['return_pct'] = df['closed_pnl'] / df['notional'].replace({0: np.nan})\n",
    "\n",
    "# profitable target\n",
    "df['profitable'] = (df['closed_pnl'] > 0).astype(int)\n",
    "\n",
    "# normalized size_usd\n",
    "if 'notional' in df.columns:\n",
    "    df['size_usd_norm'] = (df['notional'] - df['notional'].mean()) / df['notional'].std()\n",
    "\n",
    "# Prepare final ML features list (adjustable)\n",
    "features = ['_sentiment_num','sentiment_lag_1','sentiment_lag_2','sentiment_roll_7','side_num','size_usd_norm']\n",
    "ml_df = df.dropna(subset=['_sentiment_num','profitable'] + [c for c in ['size_usd_norm'] if c in df.columns], how='any').copy()\n",
    "\n",
    "print(\"\\nML dataframe shape (rows with required features):\", ml_df.shape)\n",
    "print(\"\\nFeature sample:\")\n",
    "display(ml_df[features + ['profitable','Account','trade_date']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e50b9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'value' column as numeric sentiment where available.\n",
      "Classification uniques (first 20): ['nan']\n",
      "No useful text mapping for classification found.\n",
      "Rows with numeric sentiment after fix: 0 / 211224 (0.00%)\n",
      "\n",
      "ML dataframe shape (rows kept): (0, 36)\n",
      "Example feature summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_sentiment_num</th>\n",
       "      <th>sentiment_lag_1</th>\n",
       "      <th>sentiment_lag_2</th>\n",
       "      <th>sentiment_roll_7</th>\n",
       "      <th>side_num</th>\n",
       "      <th>size_usd_norm</th>\n",
       "      <th>profitable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [_sentiment_num, sentiment_lag_1, sentiment_lag_2, sentiment_roll_7, side_num, size_usd_norm, profitable]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment numeric summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    0.0\n",
       "mean     NaN\n",
       "std      NaN\n",
       "min      NaN\n",
       "25%      NaN\n",
       "50%      NaN\n",
       "75%      NaN\n",
       "max      NaN\n",
       "Name: _sentiment_num, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Return pct summary (trimmed):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    0.0\n",
       "mean     NaN\n",
       "std      NaN\n",
       "min      NaN\n",
       "25%      NaN\n",
       "50%      NaN\n",
       "75%      NaN\n",
       "max      NaN\n",
       "Name: return_pct, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique accounts in ML dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Fix sentiment source and recreate features (copy & run)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = merged.copy()\n",
    "\n",
    "# Prefer numeric 'value' column for sentiment (common in Fear&Greed data)\n",
    "if 'value' in df.columns:\n",
    "    df['_sentiment_num'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "    print(\"Using 'value' column as numeric sentiment where available.\")\n",
    "else:\n",
    "    df['_sentiment_num'] = np.nan\n",
    "\n",
    "# If numeric not available for some rows, try mapping 'classification' text\n",
    "if df['_sentiment_num'].isna().any() and 'classification' in df.columns:\n",
    "    # show unique classification values\n",
    "    uniq = pd.Series(df['classification'].unique()).astype(str)\n",
    "    print(\"Classification uniques (first 20):\", uniq.head(20).tolist())\n",
    "    # mapping\n",
    "    mapping = {}\n",
    "    for val in uniq:\n",
    "        v = str(val).lower()\n",
    "        if 'greed' in v:\n",
    "            mapping[val] = 1\n",
    "        elif 'fear' in v:\n",
    "            mapping[val] = -1\n",
    "        elif 'neutral' in v or 'none' in v:\n",
    "            mapping[val] = 0\n",
    "    if mapping:\n",
    "        df['_sentiment_num'] = df['_sentiment_num'].fillna(df['classification'].map(mapping))\n",
    "        print(\"Applied classification->numeric mapping (example):\", mapping)\n",
    "    else:\n",
    "        print(\"No useful text mapping for classification found.\")\n",
    "\n",
    "# Count how many rows have sentiment now\n",
    "n_with_sent = df['_sentiment_num'].notna().sum()\n",
    "print(f\"Rows with numeric sentiment after fix: {n_with_sent} / {len(df)} ({n_with_sent/len(df):.2%})\")\n",
    "\n",
    "# Ensure timestamp column exists and is datetime\n",
    "ts_col = 'Timestamp' if 'Timestamp' in df.columns else ('Timestamp IST' if 'Timestamp IST' in df.columns else None)\n",
    "if ts_col is None:\n",
    "    raise KeyError(\"No timestamp column found.\")\n",
    "df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce')\n",
    "\n",
    "# sort for lagging\n",
    "df = df.sort_values(['Account', ts_col])\n",
    "\n",
    "# create lag and rolling features on _sentiment_num (per account)\n",
    "df['sentiment_lag_1'] = df.groupby('Account')['_sentiment_num'].shift(1)\n",
    "df['sentiment_lag_2'] = df.groupby('Account')['_sentiment_num'].shift(2)\n",
    "df['sentiment_roll_7'] = df.groupby('Account')['_sentiment_num'].rolling(7, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "# side encoding\n",
    "df['side_num'] = df['Side'].map({'Buy':1,'Sell':-1}).fillna(0)\n",
    "\n",
    "# ensure notional exists (you had it earlier)\n",
    "if 'notional' not in df.columns:\n",
    "    if 'Size USD' in df.columns:\n",
    "        df['notional'] = pd.to_numeric(df['Size USD'], errors='coerce')\n",
    "    else:\n",
    "        df['notional'] = (pd.to_numeric(df.get('Execution Price', np.nan), errors='coerce').abs()\n",
    "                         * pd.to_numeric(df.get('Size Tokens', np.nan), errors='coerce').abs())\n",
    "\n",
    "# closed_pnl numeric\n",
    "df['closed_pnl'] = pd.to_numeric(df.get('Closed PnL', df.get('closed_pnl', np.nan)), errors='coerce')\n",
    "\n",
    "# target and return\n",
    "df['return_pct'] = df['closed_pnl'] / df['notional'].replace({0: np.nan})\n",
    "df['profitable'] = (df['closed_pnl'] > 0).astype(int)\n",
    "\n",
    "# normalized size\n",
    "df['size_usd_norm'] = (df['notional'] - df['notional'].mean()) / df['notional'].std()\n",
    "\n",
    "# Build ML dataframe: require numeric sentiment + notional + profitable\n",
    "ml_df = df.dropna(subset=['_sentiment_num','notional','profitable']).copy()\n",
    "\n",
    "print(\"\\nML dataframe shape (rows kept):\", ml_df.shape)\n",
    "print(\"Example feature summary:\")\n",
    "display(ml_df[['_sentiment_num','sentiment_lag_1','sentiment_lag_2','sentiment_roll_7','side_num','size_usd_norm','profitable']].head(10))\n",
    "\n",
    "# Basic distributions to inspect\n",
    "print(\"\\nSentiment numeric summary:\")\n",
    "display(ml_df['_sentiment_num'].describe())\n",
    "\n",
    "print(\"\\nReturn pct summary (trimmed):\")\n",
    "display(ml_df['return_pct'].describe())\n",
    "\n",
    "# Also show how many unique accounts are available for ML\n",
    "print(\"\\nUnique accounts in ML dataset:\", ml_df['Account'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6141242c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14229   NaN\n",
      "14230   NaN\n",
      "14231   NaN\n",
      "14232   NaN\n",
      "14233   NaN\n",
      "14234   NaN\n",
      "14235   NaN\n",
      "14236   NaN\n",
      "14237   NaN\n",
      "14238   NaN\n",
      "14239   NaN\n",
      "14240   NaN\n",
      "14241   NaN\n",
      "14242   NaN\n",
      "14243   NaN\n",
      "14244   NaN\n",
      "14245   NaN\n",
      "14246   NaN\n",
      "14247   NaN\n",
      "14248   NaN\n",
      "Name: value, dtype: float64\n",
      "float64\n",
      "[nan]\n"
     ]
    }
   ],
   "source": [
    "print(df['value'].head(20))\n",
    "print(df['value'].dtype)\n",
    "print(df['classification'].unique()[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a47adaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG columns: ['timestamp', 'value', 'classification', 'fg_datetime', 'fg_date']\n",
      "\n",
      "FG head (first 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "      <th>classification</th>\n",
       "      <th>fg_datetime</th>\n",
       "      <th>fg_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1517463000</td>\n",
       "      <td>30</td>\n",
       "      <td>Fear</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2018-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1517549400</td>\n",
       "      <td>15</td>\n",
       "      <td>Extreme Fear</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>2018-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1517635800</td>\n",
       "      <td>40</td>\n",
       "      <td>Fear</td>\n",
       "      <td>2018-02-03</td>\n",
       "      <td>2018-02-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1517722200</td>\n",
       "      <td>24</td>\n",
       "      <td>Extreme Fear</td>\n",
       "      <td>2018-02-04</td>\n",
       "      <td>2018-02-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1517808600</td>\n",
       "      <td>11</td>\n",
       "      <td>Extreme Fear</td>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>2018-02-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1517895000</td>\n",
       "      <td>8</td>\n",
       "      <td>Extreme Fear</td>\n",
       "      <td>2018-02-06</td>\n",
       "      <td>2018-02-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1517981400</td>\n",
       "      <td>36</td>\n",
       "      <td>Fear</td>\n",
       "      <td>2018-02-07</td>\n",
       "      <td>2018-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1518067800</td>\n",
       "      <td>30</td>\n",
       "      <td>Fear</td>\n",
       "      <td>2018-02-08</td>\n",
       "      <td>2018-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1518154200</td>\n",
       "      <td>44</td>\n",
       "      <td>Fear</td>\n",
       "      <td>2018-02-09</td>\n",
       "      <td>2018-02-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1518240600</td>\n",
       "      <td>54</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2018-02-10</td>\n",
       "      <td>2018-02-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  value classification fg_datetime     fg_date\n",
       "0  1517463000     30           Fear  2018-02-01  2018-02-01\n",
       "1  1517549400     15   Extreme Fear  2018-02-02  2018-02-02\n",
       "2  1517635800     40           Fear  2018-02-03  2018-02-03\n",
       "3  1517722200     24   Extreme Fear  2018-02-04  2018-02-04\n",
       "4  1517808600     11   Extreme Fear  2018-02-05  2018-02-05\n",
       "5  1517895000      8   Extreme Fear  2018-02-06  2018-02-06\n",
       "6  1517981400     36           Fear  2018-02-07  2018-02-07\n",
       "7  1518067800     30           Fear  2018-02-08  2018-02-08\n",
       "8  1518154200     44           Fear  2018-02-09  2018-02-09\n",
       "9  1518240600     54        Neutral  2018-02-10  2018-02-10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FG info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2644 entries, 0 to 2643\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   timestamp       2644 non-null   int64         \n",
      " 1   value           2644 non-null   int64         \n",
      " 2   classification  2644 non-null   object        \n",
      " 3   fg_datetime     2644 non-null   datetime64[ns]\n",
      " 4   fg_date         2644 non-null   object        \n",
      "dtypes: datetime64[ns](1), int64(2), object(2)\n",
      "memory usage: 103.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value column stats (non-null counts and sample non-null rows):\n",
      "dtype: int64\n",
      "non-null count: 2644\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Date'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mnon-null count:\u001b[39m\u001b[33m\"\u001b[39m, fg[\u001b[33m'\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m'\u001b[39m].notna().sum() \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m fg.columns \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m fg.columns \u001b[38;5;129;01mand\u001b[39;00m fg[\u001b[33m'\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m'\u001b[39m].notna().sum() > \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     display(\u001b[43mfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnotna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclassification\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.head(\u001b[32m20\u001b[39m))\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mClassification unique values (up to 50):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mclassification\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m fg.columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aditya\\Desktop\\datascience\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1185\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1183\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_scalar_access(key):\n\u001b[32m   1184\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._get_value(*key, takeable=\u001b[38;5;28mself\u001b[39m._takeable)\n\u001b[32m-> \u001b[39m\u001b[32m1185\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1187\u001b[39m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[32m   1188\u001b[39m     axis = \u001b[38;5;28mself\u001b[39m.axis \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aditya\\Desktop\\datascience\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1378\u001b[39m, in \u001b[36m_LocIndexer._getitem_tuple\u001b[39m\u001b[34m(self, tup)\u001b[39m\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._multi_take_opportunity(tup):\n\u001b[32m   1376\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._multi_take(tup)\n\u001b[32m-> \u001b[39m\u001b[32m1378\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_tuple_same_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aditya\\Desktop\\datascience\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1021\u001b[39m, in \u001b[36m_LocationIndexer._getitem_tuple_same_dim\u001b[39m\u001b[34m(self, tup)\u001b[39m\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m com.is_null_slice(key):\n\u001b[32m   1019\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1021\u001b[39m retval = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mretval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[38;5;66;03m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[32m   1023\u001b[39m \u001b[38;5;66;03m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m retval.ndim == \u001b[38;5;28mself\u001b[39m.ndim\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aditya\\Desktop\\datascience\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1421\u001b[39m, in \u001b[36m_LocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1418\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[33m\"\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m   1419\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot index with multidimensional key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1421\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1423\u001b[39m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[32m   1424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aditya\\Desktop\\datascience\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1361\u001b[39m, in \u001b[36m_LocIndexer._getitem_iterable\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1358\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_key(key, axis)\n\u001b[32m   1360\u001b[39m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m keyarr, indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._reindex_with_indexers(\n\u001b[32m   1363\u001b[39m     {axis: [keyarr, indexer]}, copy=\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1364\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aditya\\Desktop\\datascience\\venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1559\u001b[39m, in \u001b[36m_LocIndexer._get_listlike_indexer\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1556\u001b[39m ax = \u001b[38;5;28mself\u001b[39m.obj._get_axis(axis)\n\u001b[32m   1557\u001b[39m axis_name = \u001b[38;5;28mself\u001b[39m.obj._get_axis_name(axis)\n\u001b[32m-> \u001b[39m\u001b[32m1559\u001b[39m keyarr, indexer = \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aditya\\Desktop\\datascience\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aditya\\Desktop\\datascience\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['Date'] not in index\""
     ]
    }
   ],
   "source": [
    "# Inspect the Fear & Greed table to see if any numeric sentiment exists\n",
    "print(\"FG columns:\", list(fg.columns))\n",
    "print(\"\\nFG head (first 10 rows):\")\n",
    "display(fg.head(10))\n",
    "\n",
    "print(\"\\nFG info():\")\n",
    "display(fg.info())\n",
    "\n",
    "print(\"\\nValue column stats (non-null counts and sample non-null rows):\")\n",
    "print(\"dtype:\", fg.get('value').dtype if 'value' in fg.columns else 'no value col')\n",
    "print(\"non-null count:\", fg['value'].notna().sum() if 'value' in fg.columns else 0)\n",
    "if 'value' in fg.columns and fg['value'].notna().sum() > 0:\n",
    "    display(fg.loc[fg['value'].notna(), ['Date','value','classification']].head(20))\n",
    "\n",
    "print(\"\\nClassification unique values (up to 50):\")\n",
    "if 'classification' in fg.columns:\n",
    "    print(pd.Series(fg['classification'].unique()).astype(str)[:50].tolist())\n",
    "else:\n",
    "    print(\"No 'classification' column.\")\n",
    "    \n",
    "print(\"\\nDate range in FG (fg_date):\")\n",
    "if 'fg_date' in fg.columns:\n",
    "    print(\"min:\", fg['fg_date'].min(), \"max:\", fg['fg_date'].max())\n",
    "else:\n",
    "    print(\"fg_date not present\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6eafe7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in merged DF: (211224, 39)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aditya\\Desktop\\datascience\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'value'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      6\u001b[39m df_merged = df.merge(\n\u001b[32m      7\u001b[39m     fg[[\u001b[33m'\u001b[39m\u001b[33mfg_date_clean\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mclassification\u001b[39m\u001b[33m'\u001b[39m]],\n\u001b[32m      8\u001b[39m     left_on=\u001b[33m'\u001b[39m\u001b[33mtrade_date\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      9\u001b[39m     right_on=\u001b[33m'\u001b[39m\u001b[33mfg_date_clean\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     10\u001b[39m     how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRows in merged DF:\u001b[39m\u001b[33m\"\u001b[39m, df_merged.shape)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRows with sentiment:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mdf_merged\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.notna().sum())\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSample sentiment rows:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m display(df_merged[[\u001b[33m'\u001b[39m\u001b[33mTimestamp\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtrade_date\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mclassification\u001b[39m\u001b[33m'\u001b[39m]].head(\u001b[32m10\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aditya\\Desktop\\datascience\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aditya\\Desktop\\datascience\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'value'"
     ]
    }
   ],
   "source": [
    "# STEP 1 — Clean and align dates\n",
    "df['trade_date'] = pd.to_datetime(df['Timestamp']).dt.date\n",
    "fg['fg_date_clean'] = pd.to_datetime(fg['fg_date']).dt.date\n",
    "\n",
    "# STEP 2 — Merge on trade_date (left join)\n",
    "df_merged = df.merge(\n",
    "    fg[['fg_date_clean', 'value', 'classification']],\n",
    "    left_on='trade_date',\n",
    "    right_on='fg_date_clean',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Rows in merged DF:\", df_merged.shape)\n",
    "print(\"Rows with sentiment:\", df_merged['value'].notna().sum())\n",
    "print(\"Sample sentiment rows:\")\n",
    "display(df_merged[['Timestamp', 'trade_date', 'value', 'classification']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "290f158a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trades columns:\n",
      " ['Account', 'Coin', 'Execution Price', 'Size Tokens', 'Size USD', 'Side', 'Timestamp IST', 'Start Position', 'Direction', 'Closed PnL', 'Transaction Hash', 'Order ID', 'Crossed', 'Fee', 'Trade ID', 'Timestamp']\n",
      "\n",
      "FG columns:\n",
      " ['timestamp', 'value', 'classification', 'date']\n",
      "\n",
      "Detected sentiment column (will use): value\n",
      "Detected FG date column: date\n",
      "\n",
      "Merged shape: (211224, 19)\n",
      "Rows with sentiment (non-null 'value'): 0 / 211224  (0.00%)\n",
      "\n",
      "Sample merged rows (first 10):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trade_date</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>1.730000e+12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>1.730000e+12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>1.730000e+12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>1.730000e+12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>1.730000e+12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>1.730000e+12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>1.730000e+12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>1.730000e+12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>1.730000e+12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>1.730000e+12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trade_date     Timestamp  value\n",
       "0  1970-01-01  1.730000e+12    NaN\n",
       "1  1970-01-01  1.730000e+12    NaN\n",
       "2  1970-01-01  1.730000e+12    NaN\n",
       "3  1970-01-01  1.730000e+12    NaN\n",
       "4  1970-01-01  1.730000e+12    NaN\n",
       "5  1970-01-01  1.730000e+12    NaN\n",
       "6  1970-01-01  1.730000e+12    NaN\n",
       "7  1970-01-01  1.730000e+12    NaN\n",
       "8  1970-01-01  1.730000e+12    NaN\n",
       "9  1970-01-01  1.730000e+12    NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A saved sample 'merged_debug_sample.csv' was written to the data folder for inspection.\n"
     ]
    }
   ],
   "source": [
    "# Safe reload + robust merge (run this cell exactly)\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT = os.path.dirname(os.getcwd())   # notebooks is current dir, parent is project root\n",
    "DATA_DIR = os.path.join(ROOT, 'data')\n",
    "trades_path = os.path.join(DATA_DIR, 'historical_data.csv')\n",
    "fg_path = os.path.join(DATA_DIR, 'fear_greed_index.csv')\n",
    "\n",
    "# 1) reload fresh\n",
    "trades = pd.read_csv(trades_path)\n",
    "fg = pd.read_csv(fg_path)\n",
    "\n",
    "# 2) show columns (so you can confirm)\n",
    "print(\"Trades columns:\\n\", list(trades.columns))\n",
    "print(\"\\nFG columns:\\n\", list(fg.columns))\n",
    "\n",
    "# 3) detect best FG sentiment column (numeric preferred)\n",
    "possible_numeric = ['value','Value','score','Score','index','Index','fg_value','fg_score']\n",
    "sent_col = None\n",
    "for c in possible_numeric:\n",
    "    if c in fg.columns:\n",
    "        sent_col = c\n",
    "        break\n",
    "\n",
    "# if not found, try to find any column name containing 'val' 'score' 'index' case-insensitive\n",
    "if sent_col is None:\n",
    "    for c in fg.columns:\n",
    "        if any(k in c.lower() for k in ['val','score','index']):\n",
    "            sent_col = c\n",
    "            break\n",
    "\n",
    "# fallback to classification if numeric absent\n",
    "if sent_col is None and 'classification' in fg.columns:\n",
    "    sent_col = 'classification'\n",
    "\n",
    "print(\"\\nDetected sentiment column (will use):\", sent_col)\n",
    "\n",
    "# 4) create fg_date_clean from any plausible date/datetime column\n",
    "date_col = None\n",
    "for c in ['fg_datetime','fg_date','Date','date','timestamp','datetime']:\n",
    "    if c in fg.columns:\n",
    "        date_col = c\n",
    "        break\n",
    "if date_col is None:\n",
    "    # try first column\n",
    "    date_col = fg.columns[0]\n",
    "print(\"Detected FG date column:\", date_col)\n",
    "\n",
    "# convert to date\n",
    "fg['fg_date_clean'] = pd.to_datetime(fg[date_col], errors='coerce').dt.date\n",
    "\n",
    "# create trade_date in trades\n",
    "# prefer 'Timestamp' then 'Timestamp IST' else try first datetime-like\n",
    "if 'Timestamp' in trades.columns:\n",
    "    trades['trade_date'] = pd.to_datetime(trades['Timestamp'], errors='coerce').dt.date\n",
    "elif 'Timestamp IST' in trades.columns:\n",
    "    trades['trade_date'] = pd.to_datetime(trades['Timestamp IST'], errors='coerce').dt.date\n",
    "else:\n",
    "    # try to parse first column that looks like datetime\n",
    "    parsed = False\n",
    "    for c in trades.columns:\n",
    "        if 'time' in c.lower() or 'date' in c.lower() or 'timestamp' in c.lower():\n",
    "            try:\n",
    "                trades['trade_date'] = pd.to_datetime(trades[c], errors='coerce').dt.date\n",
    "                parsed = True\n",
    "                print(\"Parsed trade_date from column:\", c)\n",
    "                break\n",
    "            except Exception:\n",
    "                pass\n",
    "    if not parsed:\n",
    "        raise RuntimeError(\"Could not find or parse a timestamp column in trades.\")\n",
    "\n",
    "# 5) merge on date\n",
    "if sent_col is None:\n",
    "    raise RuntimeError(\"No sentiment column detected in FG. Check fg.columns printed above.\")\n",
    "\n",
    "# choose which FG columns to bring in\n",
    "fg_keep = ['fg_date_clean', sent_col]\n",
    "fg_small = fg[fg_keep].drop_duplicates(subset=['fg_date_clean'])\n",
    "\n",
    "merged = trades.merge(fg_small, left_on='trade_date', right_on='fg_date_clean', how='left')\n",
    "\n",
    "# 6) reporting\n",
    "print(\"\\nMerged shape:\", merged.shape)\n",
    "num_with_sent = merged[sent_col].notna().sum()\n",
    "print(\"Rows with sentiment (non-null '{}'): {} / {}  ({:.2%})\".format(sent_col, num_with_sent, len(merged), num_with_sent/len(merged)))\n",
    "\n",
    "# show sample of merged columns to verify\n",
    "sample_cols = ['trade_date','Timestamp'] if 'Timestamp' in merged.columns else ['trade_date']\n",
    "sample_cols += [sent_col]\n",
    "print(\"\\nSample merged rows (first 10):\")\n",
    "display(merged[sample_cols].head(10))\n",
    "\n",
    "# expose merged to notebook session\n",
    "merged.to_csv(os.path.join(DATA_DIR, 'merged_debug_sample.csv'), index=False)\n",
    "print(\"\\nA saved sample 'merged_debug_sample.csv' was written to the data folder for inspection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1c89729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trade date sample: 0    2024-10-27\n",
      "1    2024-10-27\n",
      "2    2024-10-27\n",
      "3    2024-10-27\n",
      "4    2024-10-27\n",
      "5    2024-10-27\n",
      "6    2024-10-27\n",
      "7    2024-10-27\n",
      "8    2024-10-27\n",
      "9    2024-10-27\n",
      "Name: trade_date, dtype: object\n",
      "\n",
      "Merged shape: (211224, 20)\n",
      "Rows with sentiment: 184263 / 211224 (87.24%)\n",
      "\n",
      "Sample merged rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp_dt</th>\n",
       "      <th>trade_date</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-27 03:33:20</td>\n",
       "      <td>2024-10-27</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-27 03:33:20</td>\n",
       "      <td>2024-10-27</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-10-27 03:33:20</td>\n",
       "      <td>2024-10-27</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-10-27 03:33:20</td>\n",
       "      <td>2024-10-27</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-10-27 03:33:20</td>\n",
       "      <td>2024-10-27</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-10-27 03:33:20</td>\n",
       "      <td>2024-10-27</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-10-27 03:33:20</td>\n",
       "      <td>2024-10-27</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-10-27 03:33:20</td>\n",
       "      <td>2024-10-27</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-10-27 03:33:20</td>\n",
       "      <td>2024-10-27</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-10-27 03:33:20</td>\n",
       "      <td>2024-10-27</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Timestamp_dt  trade_date  value\n",
       "0 2024-10-27 03:33:20  2024-10-27   74.0\n",
       "1 2024-10-27 03:33:20  2024-10-27   74.0\n",
       "2 2024-10-27 03:33:20  2024-10-27   74.0\n",
       "3 2024-10-27 03:33:20  2024-10-27   74.0\n",
       "4 2024-10-27 03:33:20  2024-10-27   74.0\n",
       "5 2024-10-27 03:33:20  2024-10-27   74.0\n",
       "6 2024-10-27 03:33:20  2024-10-27   74.0\n",
       "7 2024-10-27 03:33:20  2024-10-27   74.0\n",
       "8 2024-10-27 03:33:20  2024-10-27   74.0\n",
       "9 2024-10-27 03:33:20  2024-10-27   74.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "ROOT = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = os.path.join(ROOT, \"data\")\n",
    "\n",
    "trades = pd.read_csv(os.path.join(DATA_DIR, \"historical_data.csv\"))\n",
    "fg = pd.read_csv(os.path.join(DATA_DIR, \"fear_greed_index.csv\"))\n",
    "\n",
    "# ---- FIX TRADE TIMESTAMP ----\n",
    "# Convert UNIX ms → datetime\n",
    "trades['Timestamp_dt'] = pd.to_datetime(trades['Timestamp'], unit='ms', errors='coerce')\n",
    "trades['trade_date'] = trades['Timestamp_dt'].dt.date\n",
    "\n",
    "print(\"Trade date sample:\", trades['trade_date'].head(10))\n",
    "\n",
    "# ---- Fix FG Date ----\n",
    "fg['fg_date_clean'] = pd.to_datetime(fg['date'], errors='coerce').dt.date\n",
    "\n",
    "# ---- Merge ----\n",
    "merged = trades.merge(\n",
    "    fg[['fg_date_clean', 'value']],\n",
    "    left_on='trade_date',\n",
    "    right_on='fg_date_clean',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"\\nMerged shape:\", merged.shape)\n",
    "non_null = merged['value'].notna().sum()\n",
    "print(\"Rows with sentiment:\", non_null, \"/\", len(merged), f\"({non_null/len(merged):.2%})\")\n",
    "\n",
    "print(\"\\nSample merged rows:\")\n",
    "display(merged[['Timestamp_dt','trade_date','value']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f16d8359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final ML dataset shape: (184263, 29)\n",
      "             Timestamp_dt  value  sentiment_lag_1  sentiment_ma_7  profit  profitable\n",
      "14229 2024-10-27 03:33:20   74.0              NaN            74.0     0.0           0\n",
      "14230 2024-10-27 03:33:20   74.0             74.0            74.0     0.0           0\n",
      "14231 2024-10-27 03:33:20   74.0             74.0            74.0     0.0           0\n",
      "14232 2024-10-27 03:33:20   74.0             74.0            74.0     0.0           0\n",
      "14233 2024-10-27 03:33:20   74.0             74.0            74.0     0.0           0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = merged.copy()\n",
    "\n",
    "# Ensure sorted by account and time\n",
    "df = df.sort_values([\"Account\", \"Timestamp_dt\"])\n",
    "\n",
    "# ----------------------\n",
    "# 1. Lagged sentiment\n",
    "# ----------------------\n",
    "df['sentiment_lag_1'] = df.groupby('Account')['value'].shift(1)\n",
    "df['sentiment_lag_2'] = df.groupby('Account')['value'].shift(2)\n",
    "df['sentiment_lag_3'] = df.groupby('Account')['value'].shift(3)\n",
    "\n",
    "# ----------------------\n",
    "# 2. Rolling averages\n",
    "# ----------------------\n",
    "df['sentiment_ma_7'] = df.groupby('Account')['value'].transform(lambda x: x.rolling(7, min_periods=1).mean())\n",
    "df['sentiment_ma_14'] = df.groupby('Account')['value'].transform(lambda x: x.rolling(14, min_periods=1).mean())\n",
    "\n",
    "# ----------------------\n",
    "# 3. Trader performance features\n",
    "# ----------------------\n",
    "df['profit'] = df['Closed PnL']\n",
    "df['profitable'] = (df['profit'] > 0).astype(int)\n",
    "\n",
    "# notional = price × size\n",
    "df['notional_usd'] = df['Execution Price'] * df['Size Tokens']\n",
    "\n",
    "# Buy = 1, Sell = 0\n",
    "df['is_buy'] = (df['Side'].str.lower() == 'buy').astype(int)\n",
    "\n",
    "# ----------------------\n",
    "# 4. Drop rows missing sentiment (only ~12%)\n",
    "# ----------------------\n",
    "df_ml = df.dropna(subset=['value'])\n",
    "\n",
    "print(\"Final ML dataset shape:\", df_ml.shape)\n",
    "print(df_ml[['Timestamp_dt','value','sentiment_lag_1','sentiment_ma_7','profit','profitable']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c25af0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Profitability vs Sentiment ====\n",
      "\n",
      "value\n",
      "44.0    0.415146\n",
      "50.0    0.317182\n",
      "59.0    0.000000\n",
      "69.0    0.274641\n",
      "74.0    0.451605\n",
      "84.0    0.490089\n",
      "Name: profitable, dtype: float64\n",
      "\n",
      "==== Avg Profit Amount vs Sentiment ====\n",
      "\n",
      "value\n",
      "44.0    50.047622\n",
      "50.0    22.229713\n",
      "59.0     0.000000\n",
      "69.0     0.148807\n",
      "74.0    90.504272\n",
      "84.0    25.418772\n",
      "Name: profit, dtype: float64\n",
      "\n",
      "==== Buy Ratio vs Sentiment ====\n",
      "\n",
      "value\n",
      "44.0    0.493617\n",
      "50.0    0.490828\n",
      "59.0    1.000000\n",
      "69.0    0.469856\n",
      "74.0    0.423569\n",
      "84.0    0.484200\n",
      "Name: is_buy, dtype: float64\n",
      "\n",
      "==== Sentiment Group Summary ====\n",
      "\n",
      "                    profit  profitable    is_buy\n",
      "sentiment_group                                 \n",
      "Fear             50.047622    0.415146  0.493617\n",
      "Neutral          22.220378    0.317049  0.491041\n",
      "Greed            77.843647    0.453524  0.434448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_11236\\722719111.py:40: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  summary = df.groupby('sentiment_group')[['profit', 'profitable', 'is_buy']].mean()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = df_ml.copy()\n",
    "\n",
    "# -------------------------\n",
    "# 1. Profitability vs Sentiment\n",
    "# -------------------------\n",
    "sentiment_profit = df.groupby('value')['profitable'].mean()\n",
    "\n",
    "print(\"\\n==== Profitability vs Sentiment ====\\n\")\n",
    "print(sentiment_profit.head(20))\n",
    "\n",
    "# -------------------------\n",
    "# 2. Avg profit amount vs sentiment\n",
    "# -------------------------\n",
    "sentiment_pnl = df.groupby('value')['profit'].mean()\n",
    "\n",
    "print(\"\\n==== Avg Profit Amount vs Sentiment ====\\n\")\n",
    "print(sentiment_pnl.head(20))\n",
    "\n",
    "# -------------------------\n",
    "# 3. Buy Ratio at each sentiment level\n",
    "# -------------------------\n",
    "buy_ratio = df.groupby('value')['is_buy'].mean()\n",
    "\n",
    "print(\"\\n==== Buy Ratio vs Sentiment ====\\n\")\n",
    "print(buy_ratio.head(20))\n",
    "\n",
    "# -------------------------\n",
    "# 4. Summary stats by sentiment bucket\n",
    "# Fear = 0–49, Neutral = 50–59, Greed = 60+\n",
    "# -------------------------\n",
    "df['sentiment_group'] = pd.cut(\n",
    "    df['value'],\n",
    "    bins=[0, 49, 59, 100],\n",
    "    labels=['Fear', 'Neutral', 'Greed']\n",
    ")\n",
    "\n",
    "summary = df.groupby('sentiment_group')[['profit', 'profitable', 'is_buy']].mean()\n",
    "\n",
    "print(\"\\n==== Sentiment Group Summary ====\\n\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d983cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created folders at: c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\n",
      "csv_files: c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\\csv_files\n",
      "outputs: c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\\outputs\n",
      "Saved merged dataset -> c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\\csv_files\\merged_trades.csv\n",
      "Saved final ML dataset -> c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\\csv_files\\final_ml_dataset.csv\n",
      "Saved sample top_accounts -> c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\\csv_files\\top_accounts_sample.csv\n",
      "Wrote README.md\n",
      "Wrote ds_report.md (draft)\n",
      "\n",
      "All files created. Next: we will generate the required output charts (png) and then convert ds_report.md to PDF.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Adjust these names if you used different variable names in your notebook\n",
    "# merged  -> full merged trades + sentiment\n",
    "# df_ml   -> final ML-ready DataFrame (rows with sentiment)\n",
    "\n",
    "# ------------- create folder structure -----------------\n",
    "ROOT = os.path.dirname(os.getcwd())   # parent of notebooks\n",
    "OUT_ROOT = os.path.join(ROOT, \"ds_aditya_sawant\")\n",
    "\n",
    "csv_dir = os.path.join(OUT_ROOT, \"csv_files\")\n",
    "out_dir = os.path.join(OUT_ROOT, \"outputs\")\n",
    "\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "print(\"Created folders at:\", OUT_ROOT)\n",
    "print(\"csv_files:\", csv_dir)\n",
    "print(\"outputs:\", out_dir)\n",
    "\n",
    "# ------------- save CSVs -----------------\n",
    "# Save merged dataset (full)\n",
    "merged_path = os.path.join(csv_dir, \"merged_trades.csv\")\n",
    "merged.to_csv(merged_path, index=False)\n",
    "print(\"Saved merged dataset ->\", merged_path)\n",
    "\n",
    "# Save final ML dataset (only rows with sentiment)\n",
    "final_ml_path = os.path.join(csv_dir, \"final_ml_dataset.csv\")\n",
    "df_ml.to_csv(final_ml_path, index=False)\n",
    "print(\"Saved final ML dataset ->\", final_ml_path)\n",
    "\n",
    "# Save a small sample: top accounts by avg return (helpful for report)\n",
    "try:\n",
    "    sample = df_ml.groupby('Account').agg(\n",
    "        trades_count=('profitable','count'),\n",
    "        win_rate=('profitable','mean'),\n",
    "        avg_profit=('profit','mean'),\n",
    "        avg_notional=('notional_usd','mean')\n",
    "    ).reset_index().sort_values('avg_profit', ascending=False).head(50)\n",
    "    sample_path = os.path.join(csv_dir, \"top_accounts_sample.csv\")\n",
    "    sample.to_csv(sample_path, index=False)\n",
    "    print(\"Saved sample top_accounts ->\", sample_path)\n",
    "except Exception as e:\n",
    "    print(\"Could not create top_accounts sample (no group columns?), error:\", e)\n",
    "\n",
    "# ------------- write README.md -----------------\n",
    "readme_text = f\"\"\"# ds_aditya_sawant\n",
    "\n",
    "This repository contains the assignment for the Junior Data Scientist — Trader Behavior Insights.\n",
    "\n",
    "Structure:\n",
    "- csv_files/: merged and processed CSV files\n",
    "- outputs/: charts and images (to be generated)\n",
    "- notebook_1.ipynb: the main Google Colab notebook (you should upload the .ipynb here)\n",
    "- ds_report.pdf: final summarized insights (to be generated)\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(OUT_ROOT, \"README.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(readme_text)\n",
    "print(\"Wrote README.md\")\n",
    "\n",
    "# ------------- write starter ds_report.md -----------------\n",
    "report_md = f\"\"\"# Trader Behavior Insights — Draft Report\n",
    "\n",
    "**Candidate:** Aditya (Gaami) Sawant  \n",
    "**Dataset:** Hyperliquid historical trades + Bitcoin Fear & Greed Index\n",
    "\n",
    "## Objective\n",
    "Analyze how trader behavior (profitability, risk, volume, leverage) aligns with market sentiment (Fear vs Greed).\n",
    "\n",
    "## Key preliminary findings (from EDA)\n",
    "- Greed days (value ≥60) show **higher average profit** and slightly **higher probability of profitable trades** compared to Fear days in this dataset.\n",
    "- Example aggregated numbers (approx):\n",
    "  - Fear: avg profit ≈ 50 USD, win rate ≈ 41.5%\n",
    "  - Neutral: avg profit ≈ 22 USD, win rate ≈ 31.7%\n",
    "  - Greed: avg profit ≈ 77.8 USD, win rate ≈ 45.4%\n",
    "- Buy ratio is slightly **higher in Fear** and lower in Greed (shorting/less buys in Greed).\n",
    "\n",
    "## Next steps performed in notebook_1.ipynb\n",
    "1. Data loading, parsing timestamps (Unix ms), merging daily sentiment.\n",
    "2. Feature engineering: return_pct, notional_usd, lagged sentiment (1–3 days), rolling sentiment (7/14 days).\n",
    "3. EDA and statistical tests (ANOVA / Kruskal-Wallis recommended).\n",
    "4. Predictive modeling (Logistic Regression / Random Forest) to predict profitable trades.\n",
    "5. Actionable recommendations based on results.\n",
    "\n",
    "## Recommendations (short)\n",
    "- Use sentiment as a risk overlay: reduce leverage on extreme Greed days.\n",
    "- Monitor 3-day sentiment drops as short-term reversal signals.\n",
    "- Further work: incorporate BTC price volatility, order-level holding times, and trader clustering.\n",
    "\n",
    "(Full details, code, and charts available in notebook_1.ipynb)\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(OUT_ROOT, \"ds_report.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(report_md)\n",
    "print(\"Wrote ds_report.md (draft)\")\n",
    "\n",
    "# ------------- done -------------\n",
    "print(\"\\nAll files created. Next: we will generate the required output charts (png) and then convert ds_report.md to PDF.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ae69f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_11236\\4097272859.py:36: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  grp = df.groupby('sentiment_group')['profitable'].mean().reindex(['Fear','Neutral','Greed'])\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_11236\\4097272859.py:48: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  grp2 = df.groupby('sentiment_group')['profit'].mean().reindex(['Fear','Neutral','Greed'])\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_11236\\4097272859.py:72: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  buy_ratio = df.groupby('sentiment_group')['is_buy'].mean().reindex(['Fear','Neutral','Greed'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plots:\n",
      "- c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\\outputs\\profitability_by_sentiment.png\n",
      "- c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\\outputs\\avg_profit_by_sentiment.png\n",
      "- c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\\outputs\\buy_ratio_by_sentiment.png\n",
      "- c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\\outputs\\sentiment_distribution.png\n",
      "- c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\\outputs\\top15_accounts_avg_profit.png\n"
     ]
    }
   ],
   "source": [
    "# Step B — Generate and save output charts (run this cell)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT = os.path.dirname(os.getcwd())\n",
    "OUT_ROOT = os.path.join(ROOT, \"ds_aditya_sawant\")\n",
    "OUT_DIR = os.path.join(OUT_ROOT, \"outputs\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Use df_ml if available, else try merged with value present\n",
    "if 'df_ml' in globals():\n",
    "    df = df_ml.copy()\n",
    "elif 'df' in globals():\n",
    "    df = df.copy()\n",
    "elif 'merged' in globals():\n",
    "    df = merged.copy()\n",
    "else:\n",
    "    raise RuntimeError(\"Could not find df_ml, df, or merged in the notebook namespace. Load merged data first.\")\n",
    "\n",
    "# Ensure necessary cols\n",
    "df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "df['profit'] = pd.to_numeric(df.get('profit', df.get('Closed PnL', df.get('closed_pnl'))), errors='coerce')\n",
    "df['profitable'] = (df['profit'] > 0).astype(int)\n",
    "df['is_buy'] = df.get('is_buy', (df.get('Side', '').str.lower() == 'buy').astype(int))\n",
    "\n",
    "# Create sentiment groups\n",
    "df['sentiment_group'] = pd.cut(df['value'], bins=[-1,49,59,100], labels=['Fear','Neutral','Greed'])\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Chart 1: Profitability rate by sentiment group (bar)\n",
    "plt.figure(figsize=(6,4))\n",
    "grp = df.groupby('sentiment_group')['profitable'].mean().reindex(['Fear','Neutral','Greed'])\n",
    "sns.barplot(x=grp.index, y=grp.values)\n",
    "plt.ylabel('Probability of Profitable Trade')\n",
    "plt.title('Profitability Rate by Sentiment Group')\n",
    "plt.ylim(0,1)\n",
    "plt.tight_layout()\n",
    "p1 = os.path.join(OUT_DIR, 'profitability_by_sentiment.png')\n",
    "plt.savefig(p1, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Chart 2: Average profit (USD) by sentiment group (bar)\n",
    "plt.figure(figsize=(6,4))\n",
    "grp2 = df.groupby('sentiment_group')['profit'].mean().reindex(['Fear','Neutral','Greed'])\n",
    "sns.barplot(x=grp2.index, y=grp2.values)\n",
    "plt.ylabel('Average Profit (USD)')\n",
    "plt.title('Average Profit by Sentiment Group')\n",
    "plt.tight_layout()\n",
    "p2 = os.path.join(OUT_DIR, 'avg_profit_by_sentiment.png')\n",
    "plt.savefig(p2, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Chart 3: Boxplot of return_pct by sentiment group\n",
    "if 'return_pct' in df.columns:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.boxplot(data=df, x='sentiment_group', y='return_pct', order=['Fear','Neutral','Greed'])\n",
    "    plt.ylim(np.percentile(df['return_pct'].dropna(),1), np.percentile(df['return_pct'].dropna(),99))\n",
    "    plt.title('Return % by Sentiment Group (1-99 pct shown)')\n",
    "    plt.tight_layout()\n",
    "    p3 = os.path.join(OUT_DIR, 'returnpct_box_by_sentiment.png')\n",
    "    plt.savefig(p3, dpi=150)\n",
    "    plt.close()\n",
    "else:\n",
    "    p3 = None\n",
    "\n",
    "# Chart 4: Buy ratio by sentiment group\n",
    "plt.figure(figsize=(6,4))\n",
    "buy_ratio = df.groupby('sentiment_group')['is_buy'].mean().reindex(['Fear','Neutral','Greed'])\n",
    "sns.barplot(x=buy_ratio.index, y=buy_ratio.values)\n",
    "plt.ylabel('Buy Ratio (fraction)')\n",
    "plt.title('Buy Ratio by Sentiment Group')\n",
    "plt.ylim(0,1)\n",
    "plt.tight_layout()\n",
    "p4 = os.path.join(OUT_DIR, 'buy_ratio_by_sentiment.png')\n",
    "plt.savefig(p4, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Chart 5: Sentiment value distribution (histogram)\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(df['value'].dropna(), bins=20, kde=False)\n",
    "plt.xlabel('Fear & Greed Score')\n",
    "plt.title('Distribution of Fear & Greed Scores in Dataset')\n",
    "plt.tight_layout()\n",
    "p5 = os.path.join(OUT_DIR, 'sentiment_distribution.png')\n",
    "plt.savefig(p5, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Chart 6: Top 15 accounts by avg profit (bar)\n",
    "try:\n",
    "    acct = df.groupby('Account').agg(avg_profit=('profit','mean'), trades=('profit','count')).reset_index()\n",
    "    top = acct[acct['trades']>=5].sort_values('avg_profit', ascending=False).head(15)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.barplot(data=top, x='avg_profit', y='Account')\n",
    "    plt.xlabel('Average Profit (USD)')\n",
    "    plt.title('Top 15 Accounts by Avg Profit (min 5 trades)')\n",
    "    plt.tight_layout()\n",
    "    p6 = os.path.join(OUT_DIR, 'top15_accounts_avg_profit.png')\n",
    "    plt.savefig(p6, dpi=150)\n",
    "    plt.close()\n",
    "except Exception as e:\n",
    "    p6 = None\n",
    "    print(\"Could not create top accounts chart:\", e)\n",
    "\n",
    "saved = [p for p in [p1,p2,p3,p4,p5,p6] if p]\n",
    "print(\"Saved plots:\")\n",
    "for s in saved:\n",
    "    print(\"-\", s)\n",
    "\n",
    "# expose a list for later steps\n",
    "plots_saved = saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26483191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing reportlab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote PDF: c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\\ds_report.pdf\n"
     ]
    }
   ],
   "source": [
    "# Create ds_report.pdf by embedding images and text from ds_report.md\n",
    "# Installs reportlab if missing, then writes a simple formatted PDF.\n",
    "\n",
    "import os\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "ROOT = os.path.dirname(os.getcwd())\n",
    "OUT_ROOT = os.path.join(ROOT, \"ds_aditya_sawant\")\n",
    "MD_PATH = os.path.join(OUT_ROOT, \"ds_report.md\")\n",
    "OUT_PDF = os.path.join(OUT_ROOT, \"ds_report.pdf\")\n",
    "OUT_DIR = os.path.join(OUT_ROOT, \"outputs\")\n",
    "\n",
    "# install reportlab if not present\n",
    "import importlib, sys\n",
    "pkg = importlib.util.find_spec(\"reportlab\")\n",
    "if pkg is None:\n",
    "    print(\"Installing reportlab...\")\n",
    "    !pip install reportlab --quiet\n",
    "\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.lib.units import mm\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.utils import ImageReader\n",
    "\n",
    "def write_pdf_from_md(md_path, out_pdf, images_dir):\n",
    "    # read markdown\n",
    "    with open(md_path, 'r', encoding='utf-8') as f:\n",
    "        md = f.read()\n",
    "    # naive markdown -> paragraphs (keeps headings and lists as text)\n",
    "    lines = md.splitlines()\n",
    "    # prepare canvas\n",
    "    c = canvas.Canvas(out_pdf, pagesize=A4)\n",
    "    width, height = A4\n",
    "    margin = 20*mm\n",
    "    x = margin\n",
    "    y = height - margin\n",
    "    max_width = width - 2*margin\n",
    "    line_height = 10  # pts\n",
    "    # simple font\n",
    "    c.setFont(\"Helvetica-Bold\", 14)\n",
    "    c.drawString(x, y, \"Trader Behavior Insights — Report\")\n",
    "    y -= 1.5*line_height\n",
    "    c.setFont(\"Helvetica\", 10)\n",
    "    y -= line_height\n",
    "    wrap = textwrap.TextWrapper(width=100)\n",
    "    # iterate markdown lines and write text; embed images when image filenames appear in outputs\n",
    "    for i, raw in enumerate(lines):\n",
    "        if raw.strip() == \"\":\n",
    "            y -= line_height/2\n",
    "            continue\n",
    "        # If heading\n",
    "        if raw.startswith(\"#\"):\n",
    "            level = raw.count(\"#\")\n",
    "            heading = raw.lstrip(\"#\").strip()\n",
    "            c.setFont(\"Helvetica-Bold\", 12 - min(level-1,3))\n",
    "            if y < 60:\n",
    "                c.showPage()\n",
    "                y = height - margin\n",
    "            c.drawString(x, y, heading)\n",
    "            y -= 1.2*line_height\n",
    "            c.setFont(\"Helvetica\", 10)\n",
    "            continue\n",
    "        # If image reference present (we don't require markdown image syntax; we will embed known output files)\n",
    "        # embed each image from outputs folder in sequence at logical breakpoints (first few image files)\n",
    "        # We'll attempt to place outputs in the order: profitability_by_sentiment.png, avg_profit_by_sentiment.png, returnpct_box_by_sentiment.png, buy_ratio_by_sentiment.png, sentiment_distribution.png, top15_accounts_avg_profit.png\n",
    "        # Only embed once when encountering the word \"charts\" or \"images\" or \"plots\" or at end\n",
    "        low = raw.lower()\n",
    "        if any(k in low for k in [\"charts\", \"images\", \"plots\", \"outputs\", \"figures\"]):\n",
    "            # embed images (loop)\n",
    "            img_list = [\n",
    "                \"profitability_by_sentiment.png\",\n",
    "                \"avg_profit_by_sentiment.png\",\n",
    "                \"returnpct_box_by_sentiment.png\",\n",
    "                \"buy_ratio_by_sentiment.png\",\n",
    "                \"sentiment_distribution.png\",\n",
    "                \"top15_accounts_avg_profit.png\"\n",
    "            ]\n",
    "            c.setFont(\"Helvetica-Oblique\", 9)\n",
    "            y -= line_height/2\n",
    "            for img in img_list:\n",
    "                img_path = os.path.join(images_dir, img)\n",
    "                if os.path.exists(img_path):\n",
    "                    # scale image to page width\n",
    "                    try:\n",
    "                        im = ImageReader(img_path)\n",
    "                        iw, ih = im.getSize()\n",
    "                        scale = min((max_width) / iw, (height/3) / ih)\n",
    "                        w = iw * scale\n",
    "                        h = ih * scale\n",
    "                        if y - h < margin:\n",
    "                            c.showPage()\n",
    "                            y = height - margin\n",
    "                        c.drawImage(im, x, y-h, width=w, height=h)\n",
    "                        y -= h + line_height/2\n",
    "                    except Exception as e:\n",
    "                        print(\"Could not embed\", img_path, \":\", e)\n",
    "            c.setFont(\"Helvetica\", 10)\n",
    "            continue\n",
    "        # otherwise normal paragraph (wrap long lines)\n",
    "        wrapped = wrap.wrap(raw)\n",
    "        for part in wrapped:\n",
    "            if y < 60:\n",
    "                c.showPage()\n",
    "                y = height - margin\n",
    "            c.drawString(x, y, part)\n",
    "            y -= line_height\n",
    "    c.save()\n",
    "    print(\"Wrote PDF:\", out_pdf)\n",
    "\n",
    "# run\n",
    "if not os.path.exists(MD_PATH):\n",
    "    raise FileNotFoundError(\"ds_report.md not found at: \" + MD_PATH)\n",
    "write_pdf_from_md(MD_PATH, OUT_PDF, OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6403a3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied notebook to: c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\\notebook_1.ipynb\n"
     ]
    }
   ],
   "source": [
    "import shutil, os\n",
    "ROOT = os.path.dirname(os.getcwd())              # parent of notebooks\n",
    "SRC_NOTEBOOK = os.path.join(ROOT, \"notebooks\", \"Trader_Behavior_Analysis.ipynb\")\n",
    "DEST = os.path.join(ROOT, \"ds_aditya_sawant\", \"notebook_1.ipynb\")\n",
    "\n",
    "if os.path.exists(SRC_NOTEBOOK):\n",
    "    shutil.copyfile(SRC_NOTEBOOK, DEST)\n",
    "    print(\"Copied notebook to:\", DEST)\n",
    "else:\n",
    "    print(\"Source notebook not found at:\", SRC_NOTEBOOK)\n",
    "    print(\"If your notebook has a different name, set SRC_NOTEBOOK to that path and re-run.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0e8febd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is 921E-3F6D\n",
      "\n",
      " Directory of c:\\Users\\Aditya\\Desktop\\datascience\\notebooks\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File Not Found\n"
     ]
    }
   ],
   "source": [
    "!dir ds_aditya_sawant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "449c66ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is 921E-3F6D\n",
      "\n",
      " Directory of c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\n",
      "\n",
      "24-11-2025  07:03 PM    <DIR>          .\n",
      "24-11-2025  06:59 PM    <DIR>          ..\n",
      "24-11-2025  06:59 PM    <DIR>          csv_files\n",
      "24-11-2025  06:59 PM             1,566 ds_report.md\n",
      "24-11-2025  07:02 PM           332,285 ds_report.pdf\n",
      "24-11-2025  07:03 PM                 0 notebook_1.ipynb\n",
      "24-11-2025  07:00 PM    <DIR>          outputs\n",
      "24-11-2025  06:59 PM               382 README.md\n",
      "               4 File(s)        334,233 bytes\n",
      "               4 Dir(s)  14,680,162,304 bytes free\n",
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is 921E-3F6D\n",
      "\n",
      " Directory of c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\\csv_files\n",
      "\n",
      "24-11-2025  06:59 PM    <DIR>          .\n",
      "24-11-2025  07:03 PM    <DIR>          ..\n",
      "24-11-2025  06:59 PM        62,257,978 final_ml_dataset.csv\n",
      "24-11-2025  06:59 PM        60,929,492 merged_trades.csv\n",
      "24-11-2025  06:59 PM             3,424 top_accounts_sample.csv\n",
      "               3 File(s)    123,190,894 bytes\n",
      "               2 Dir(s)  14,680,162,304 bytes free\n",
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is 921E-3F6D\n",
      "\n",
      " Directory of c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\\outputs\n",
      "\n",
      "24-11-2025  07:00 PM    <DIR>          .\n",
      "24-11-2025  07:03 PM    <DIR>          ..\n",
      "24-11-2025  07:00 PM            30,614 avg_profit_by_sentiment.png\n",
      "24-11-2025  07:00 PM            26,867 buy_ratio_by_sentiment.png\n",
      "24-11-2025  07:00 PM            29,679 profitability_by_sentiment.png\n",
      "24-11-2025  07:00 PM            37,369 sentiment_distribution.png\n",
      "24-11-2025  07:00 PM           149,870 top15_accounts_avg_profit.png\n",
      "               5 File(s)        274,399 bytes\n",
      "               2 Dir(s)  14,680,162,304 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir \"c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\"\n",
    "!dir \"c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\\csv_files\"\n",
    "!dir \"c:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\\outputs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03e6ed01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n",
      "Size (bytes): 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "notebook_path = r\"C:\\Users\\Aditya\\Desktop\\datascience\\ds_aditya_sawant\\notebook_1.ipynb\"\n",
    "print(\"Exists:\", os.path.exists(notebook_path))\n",
    "print(\"Size (bytes):\", os.path.getsize(notebook_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b273b43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
